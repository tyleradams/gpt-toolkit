#!/usr/bin/env python3

import os
import click
import openai
import sys
import json
import uuid
import readline

def gpt(content, model, n, temperature, top_p, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, conversation_history):
    d = openai.ChatCompletion.create(
        model=model,
        messages=[*conversation_history, {"role": "user", "content": content}],
        n=n,
        temperature=temperature,
        top_p=top_p,
        stream=stream,
        stop=stop,
        max_tokens=max_tokens,
        presence_penalty=presence_penalty,
        frequency_penalty=frequency_penalty,
        logit_bias=json.loads(logit_bias),
        user=user
    )
    return d.choices[0]["message"]["content"]

@click.command(context_settings={"ignore_unknown_options": True,"allow_extra_args": True})
@click.option('-m', '--model', default='gpt-3.5-turbo', help='Specifies the AI model to use.')
@click.option('-4', 'model_set_to_gpt4', is_flag=True, help='Sets the model to gpt-4')
@click.option('-n', type=int, default=1, help='Specifies how many chat completion choices to generate for each input message.')
@click.option('--temperature', type=float, default=1.0, help='Sets the sampling temperature between 0 and 2.')
@click.option('--top-p', type=float, default=1.0, help='Sets the alternative to sampling with temperature, called nucleus sampling.')
@click.option('--stream', type=bool, default=False, help='Sets whether partial message deltas will be sent.')
@click.option('--stop', type=str, default=None, help='Sets up to 4 sequences where the API will stop generating further tokens.')
@click.option('--max-tokens', type=int, default=None, help='Defines maximum number of tokens to generate in the chat completion.')
@click.option('--presence-penalty', type=float, default=0.0, help='Sets the penalty for new tokens based on whether they appear in the text so far.')
@click.option('--frequency-penalty', type=float, default=0.0, help='Sets the penalty for new tokens based on their existing frequency in the text so far.')
@click.option('--logit-bias', type=str, default="{}", help='Allows to modify the likelihood of specified tokens appearing in the completion.')
@click.option('--user', type=str, default=str(uuid.uuid4()), help='An identifier representing your end-user, serves to monitor and detect abuse.')
@click.option('--repl', is_flag=True, help='Enable REPL mode.')
@click.option('-f', type=str, help='Use file instead of stdin')
def main(model, model_set_to_gpt4, repl, f, n, temperature, top_p, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user):
    if model_set_to_gpt4:
        model='gpt-4'
    if repl and f:
        raise Exception("Cannot enable both repl mode and use a file input yet")
    if repl and not f:
        conversation_history = []
        while True:
            content = input("> ")
            if content.lower() == "exit":
                return
            assistant_response = gpt(content, model, n, temperature, top_p, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, conversation_history)
            conversation_history.append({"role": "user", "content": content})
            conversation_history.append({"role": "assistant", "content": assistant_response})
            print(assistant_response)
    elif not repl and f:
        with open(f) as fp:
            message = fp.read()
    elif not repl and not f:
        message = sys.stdin.read()
    else:
        raise Exception(f"Unreachable code exception {repl} {f}")

    print(gpt(message, model, n, temperature, top_p, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, []))


if __name__ == "__main__":
    openai.organization = os.getenv("OPENAI_ORGANIZATION")
    openai.api_key = os.getenv("OPENAI_API_KEY")
    main()  # pylint: disable=no-value-for-parameter
